pipeline {
  agent any
  stages {
    stage('Git CLoning') {
      steps {
        git(url: 'https://github.com/yaseensec/Jenkinsfiles.git', branch: 'master')
      }
    }

    stage('Build') {
      parallel {
        stage('Build') {
          steps {
            sh 'ls /tmp'
            sh 'ls -al /tmp'
          }
        }

        stage('Test Step') {
          steps {
            sleep 10
            sh 'ls -al /tmp'
          }
        }

      }
    }

    stage('Test') {
      parallel {
        stage('Test') {
          steps {
            sh 'ls /usr/bin'
          }
        }

        stage('Test1') {
          steps {
            sh 'ls -al /var/lib'
          }
        }

      }
    }

    stage('docker rmi') {
      steps {
        sh '''docker images
'''
        sh 'docker rmi $(docker images --format \'{{.Repository}}:{{.Tag}}\' | grep \'014357505841.dkr.ecr.ap-south-1.amazonaws.com/erp\')'
      }
    }

  }
}

pipeline {
    agent any

    stages {
        stage('Get Parameters') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map

                    // Loop through all available parameters
                    params.each { paramName, paramValue ->
                        if (paramValue) {
                            paramMap[paramName] = paramValue
                        }
                    }

                    // Print the parameter map for verification
                    echo "Parameter Map: ${paramMap}"

                    // Pass paramMap to the next stage
                    build job: 'NextJob', parameters: [string(name: 'paramMap', value: paramMap.inspect())]
                }
            }
        }
    }
}


pipeline {
    agent any

    stages {
        stage('Next Stage') {
            steps {
                script {
                    def paramMap = evaluate("return ${params.paramMap}")

                    // Loop through paramMap and access parameter names and values
                    paramMap.each { paramName, paramValue ->
                        echo "Parameter Name: ${paramName}"
                        echo "Parameter Value: ${paramValue}"
                        
                        // Perform actions based on parameter name and value
                        // Example: if (paramName == 'global') { /* do something */ }
                    }
                }
            }
        }
    }
}



pipeline {
    agent any

    stages {
        stage('Run Ansible Playbook for Filled Parameters') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map

                    // Loop through all available parameters
                    params.each { paramName, paramValue ->
                        if (paramValue) {
                            paramMap[paramName] = paramValue
                        }
                    }

                    // Print the parameter map for verification
                    echo "Parameter Map: ${paramMap}"

                    // Loop through paramMap and run Ansible playbook for each filled parameter
                    paramMap.each { paramName, paramValue ->
                        // Run Ansible playbook with parameterName and parameterValue as variables
                        sh """
                            ansible-playbook your_playbook.yml \
                            -e "paramName=${paramName}" \
                            -e "paramValue=${paramValue}"
                        """
                    }
                }
            }
        }
    }
}

pipeline {
    agent any

    stages {
        stage('Run Ansible Playbook for Filled String Parameters') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map

                    // Filter and process only string parameters
                    def stringParams = params.findAll { paramName, paramValue ->
                        paramValue instanceof String
                    }

                    // Loop through the filtered string parameters
                    stringParams.each { paramName, paramValue ->
                        paramMap[paramName] = paramValue
                    }

                    // Print the parameter map for verification
                    echo "Parameter Map: ${paramMap}"

                    // Loop through paramMap and run Ansible playbook for each filled string parameter
                    paramMap.each { paramName, paramValue ->
                        // Run Ansible playbook with parameterName and parameterValue as variables
                        sh """
                            ansible-playbook your_playbook.yml \
                            -e "paramName=${paramName}" \
                            -e "paramValue=${paramValue}"
                        """
                    }
                }
            }
        }
    }
}


pipeline {
    agent any

    stages {
        stage('Run Ansible Playbooks') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map
                    def webAppVersion

                    // Loop through all available parameters
                    params.each { paramName, paramValue ->
                        if (paramValue instanceof String && paramValue != '') {
                            paramMap[paramName] = paramValue
                            if (paramName == 'webapp') {
                                webAppVersion = paramValue
                            }
                        }
                    }

                    // Print the parameter map for verification
                    echo "Parameter Map: ${paramMap}"

                    if ('webapp' in paramMap) {
                        // Run the webapp playbook with paramName and paramValue as environment variables
                        sh """
                            ansible-playbook webapp_playbook.yml \
                            -e "paramName=webapp" \
                            -e "paramValue=${paramValue}" \
                            -e "webapp_version=${paramValue}"
                        """
                    } else if (paramMap) {
                        // Run the default playbook for other parameters
                        sh "ansible-playbook default_playbook.yml"
                    }
                }
            }
        }
    }
}




pipeline {
    agent any

    stages {
        stage('Run Ansible Playbooks') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map
                    def webAppVersion

                    // Loop through all available parameters
                    params.each { paramName, paramValue ->
                        if (paramValue instanceof String && paramValue != '') {
                            paramMap[paramName] = paramValue
                            if (paramName == 'webapp') {
                                webAppVersion = paramValue
                            }
                        }
                    }

                    // Print the parameter map for verification
                    echo "Parameter Map: ${paramMap}"

                    if ('webapp' in paramMap) {
                        // Run the webapp playbook with paramName and paramValue as environment variables
                        sh """
                            ansible-playbook webapp_playbook.yml \
                            -e "paramName=${paramName}" \
                            -e "paramValue=${paramValue}" \
                            -e "webapp_version=${webAppVersion}"
                        """
                    } else if (paramMap) {
                        // Run the default playbook for other parameters
                        sh "ansible-playbook default_playbook.yml"
                    }
                }
            }
        }
    }
}



pipeline {
    agent any

    stages {
        stage('Checkout') {
            steps {
                // Git checkout step
                // ...
            }
        }

        stage('Add SonarQube and Jacoco Plugins') {
            steps {
                script {
                    // Read the pom.xml file
                    def pomXml = readFile('path/to/your/pom.xml')

                    // Define the plugin blocks to add
                    def sonarPluginBlock = '''
                        <plugin>
                            <groupId>org.sonarsource.scanner.maven</groupId>
                            <artifactId>sonar-maven-plugin</artifactId>
                            <version>3.8.0.2131</version>
                        </plugin>
                    '''

                    def jacocoPluginBlock = '''
                        <plugin>
                            <groupId>org.jacoco</groupId>
                            <artifactId>jacoco-maven-plugin</artifactId>
                            <version>0.8.7</version>
                        </plugin>
                    '''

                    // Use awk to insert the plugin blocks into the pom.xml
                    def modifiedPomXml = pomXml.replaceAll('<\/build>', sonarPluginBlock + '\n<\/build>')
                    modifiedPomXml = modifiedPomXml.replaceAll('<\/project>', jacocoPluginBlock + '\n<\/project>')

                    // Write the modified pom.xml back to the workspace
                    writeFile(file: 'path/to/your/pom.xml', text: modifiedPomXml)
                }
            }
        }

        stage('Build and SonarQube Scan') {
            steps {
                // Maven build step
                sh 'mvn clean install'

                // Execute SonarQube scan
                sh 'mvn sonar:sonar'
            }
        }
    }

    // ...
}


def jacocoPluginBlock = '''
                <plugin>
                    <groupId>org.jacoco</groupId>
                    <artifactId>jacoco-maven-plugin</artifactId>
                    <version>0.8.7</version>
                    <executions>
                        <execution>
                            <goals>
                                <goal>prepare-agent</goal>
                            </goals>
                        </execution>
                        <execution>
                            <id>report</id>
                            <phase>prepare-package</phase>
                            <goals>
                                <goal>report</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>


def paramMap = [:]  // Initialize an empty map
// Populate paramMap with your key-value pairs

def playbookSteps = [:]  // Initialize a map to store playbook steps

paramMap.each { paramName, paramValue ->
    // Define the name for this step (e.g., "Run Playbook - paramName")
    def stepName = "Run Playbook - ${paramName}"

    // Define the shell script to run the Ansible playbook with the parameters
    def shellScript = """
        ansible-playbook your_playbook.yml \\
        -e "paramName=${paramName}" \\
        -e "paramValue=${paramValue}"
    """

    // Store the step in the playbookSteps map
    playbookSteps[stepName] = {
        build job: 'Your-Jenkins-Job-Name', parameters: [
            string(name: 'ShellScript', value: shellScript)
        ]
    }
}

parallel playbookSteps



pipeline {
    agent any
    stages {
        stage('Read File and Set Variables') {
            steps {
                script {
                    def fileContent = readFile('PWD.txt').trim()
                    def fileLines = fileContent.tokenize('\n') // Split by newline if multiple lines

                    // You can now set the content as environment variables
                    for (int i = 0; i < fileLines.size(); i++) {
                        def parts = fileLines[i].tokenize('=')
                        if (parts.size() == 2) {
                            env[parts[0]] = parts[1]
                        }
                    }
                }
            }
        }
        stage('Use Loaded Variables') {
            steps {
                echo "Value from PWD.txt: ${env.YOUR_VARIABLE_NAME}" // Access the loaded variable
            }
        }
    }
}



27oct

pipeline {
    agent any

    stages {
        stage('Run Playbooks') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map
                    // Populate paramMap with your key-value pairs

                    // Check if paramMap contains the "props" parameter
                    if (paramMap.containsKey('props')) {
                        // Execute playbook for "props" parameter
                        def propsParamValue = paramMap.props
                        ansiblePlaybook(
                            playbook: 'props_playbook.yml',
                            inventory: 'your_inventory.ini',
                            extras: "-e paramName=props -e paramValue=${propsParamValue}"
                        )
                        // Remove "props" parameter from paramMap
                        paramMap.remove('props')
                    }

                    def playbookSteps = [:]  // Initialize a map to store playbook steps
                    def maxParallel = 2  // Maximum number of parallel steps

                    paramMap.each { paramName, paramValue ->
                        // Define the name for this step (e.g., "Run Playbook - paramName")
                        def stepName = "Run Playbook - ${paramName}"

                        // Define the inventory file and extras for the playbook step
                        def inventoryFile = 'your_inventory.ini'
                        def extras = "-e paramName=${paramName} -e paramValue=${paramValue}"

                        // Define the playbook step for this parameter
                        playbookSteps[stepName] = {
                            ansiblePlaybook(
                                playbook: 'your_playbook.yml',
                                inventory: inventoryFile,
                                extras: extras
                            )
                        }
                    }

                    // Split paramMap into sets of at most 2 parameters
                    def paramMapSets = paramMap.collate(maxParallel)

                    // Execute playbook steps for each paramMap set
                    paramMapSets.each { paramMapSet ->
                        parallel paramMapSet.collectEntries { paramName, _ ->
                            ["Run Playbook - $paramName": playbookSteps["Run Playbook - $paramName"]]
                        }
                    }
                }
            }
        }
    }
}



pipeline {
    agent any

    stages {
        stage('Run Playbooks') {
            steps {
                script {
                    def paramMap = [:]  // Initialize an empty map
                    // Populate paramMap with your key-value pairs

                    def maxParallel = 2  // Maximum number of parallel steps

                    paramMap.each { paramName, paramValue ->
                        // Define the name for this step (e.g., "Run Playbook - paramName")
                        def stepName = "Run Playbook - ${paramName}"

                        // Define the inventory file and extras for the playbook step
                        def inventoryFile = 'your_inventory.ini'
                        def extras = "-e paramName=${paramName} -e paramValue=${paramValue}"

                        // Define the playbook step for this parameter
                        steps[stepName] = {
                            sh "echo -e \"\\u001B[32m$stepName\\u001B[0m\""  // Add custom label with green color
                            ansiblePlaybook(
                                playbook: 'your_playbook.yml',
                                inventory: inventoryFile,
                                extras: extras
                            )
                        }
                    }

                    // Split paramMap into sets of at most 2 parameters
                    def paramMapSets = paramMap.collate(maxParallel)

                    // Execute playbook steps for each paramMap set
                    paramMapSets.each { paramMapSet ->
                        paramMapSet.each { paramName, _ ->
                            def stepName = "Run Playbook - ${paramName}"
                            parallel "$stepName": steps[stepName]
                        }
                    }
                }
            }



// Define a function to wrap text in ANSI color codes
def colorize(text, colorCode) {
    return "\u001B[${colorCode}m${text}\u001B[0m"
}

// Define color codes for blue and green
def blueColorCode = 34 // Blue
def greenColorCode = 32 // Green

// Initialize color index to determine the color for each paramMapSet
def colorIndex = 0

// Execute playbook steps for each paramMapSet
paramMapSets.each { paramMapSet ->
    parallel paramMapSet.collect { paramName, paramValue ->
        // Determine the color code for the current paramMapSet (round-robin between blue and green)
        def currentColorCode = colorIndex % 2 == 0 ? blueColorCode : greenColorCode

        // Get the colorized output using ANSI color codes
        def coloredParamName = colorize(paramName, currentColorCode)
        def coloredParamValue = colorize(paramValue, currentColorCode)

        // Print the colored output
        echo "Running Playbook - $coloredParamName: $coloredParamValue"

        // Execute playbook step here
        def playbookStep = playbookSteps["Run Playbook - $paramName"]
        if (playbookStep) {
            parallel(
                "Execute Playbook - $paramName": {
                    // Run playbook step for this paramMapSet
                    // You should replace this with your actual playbook execution logic
                    sh playbookStep
                }
            )
        }

        // Increment the color index for the next paramMapSet
        colorIndex++
    }
}

        }
    }
}


def paramMapSets = []
def currentSet = [:]

paramMap.each { paramName, paramValue ->
    currentSet[paramName] = paramValue
    if (currentSet.size() == maxParallel) {
        paramMapSets.add(currentSet)
        currentSet = [:]
    }
}

if (currentSet.size() > 0) {
    paramMapSets.add(currentSet)
}

// Execute playbook steps for each paramMap set
paramMapSets.each { paramMapSet ->
    parallel paramMapSet.collect { paramName, paramValue ->
        ["Run Playbook - $paramName": playbookSteps["Run Playbook - $paramName"]]
    }
}



// Define a function to wrap text in ANSI color codes
def colorize(text, colorCode) {
    return "\u001B[${colorCode}m${text}\u001B[0m"
}

// Define color codes for blue and green
def blueColorCode = 34 // Blue
def greenColorCode = 32 // Green

// Initialize color counters
def colorCounter = 0

// ... (Your existing code)

// Execute playbook steps for each paramMap set
paramMapSets.each { paramMapSet ->
    parallel paramMapSet.collect { paramName, paramValue ->
        // Determine the color code for the current step (round-robin between blue and green)
        def currentColorCode = colorCounter % 2 == 0 ? blueColorCode : greenColorCode

        // Get the colorized output using ANSI color codes
        def coloredOutput = colorize("Running Playbook - $paramName: $paramValue", currentColorCode)

        // Print the colored output
        echo coloredOutput

        // Increment the color counter for the next step
        colorCounter++

        // Execute playbook step here (you might need to update your playbook execution logic)
    }
}


// Define a function to wrap text in ANSI color codes
def colorize(text, colorCode) {
    return "\u001B[${colorCode}m${text}\u001B[0m"
}

// Define color codes for blue and green
def blueColorCode = 34 // Blue
def greenColorCode = 32 // Green

// Initialize color index to determine the color for each paramMapSet
def colorIndex = 0

// Execute playbook steps for each paramMapSet
paramMapSets.each { paramMapSet ->
    parallel paramMapSet.collect { paramName, paramValue ->
        // Determine the color code for the current paramMapSet (round-robin between blue and green)
        def currentColorCode = colorIndex % 2 == 0 ? blueColorCode : greenColorCode

        // Get the colorized output using ANSI color codes
        def coloredParamName = colorize(paramName, currentColorCode)
        def coloredParamValue = colorize(paramValue, currentColorCode)

        // Print the colored output
        echo "Running Playbook - $coloredParamName: $coloredParamValue"

        // Execute playbook step here
        def playbookStep = playbookSteps["Run Playbook - $paramName"]
        if (playbookStep) {
            parallel(
                "Execute Playbook - $paramName": {
                    // Run playbook step for this paramMapSet
                    // You should replace this with your actual playbook execution logic
                    sh playbookStep
                }
            )
        }

        // Increment the color index for the next paramMapSet
        colorIndex++
    }
}



pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                script {
                    def paramMap = [param1: 'value1', param2: 'value2']
                    def maxParallel = 2

                    def colorCodes = [34, 32] // Blue and Green ANSI color codes

                    def colorIndex = 0

                    // Simulate paramMapSets
                    def paramMapSets = []
                    def currentSet = [:]

                    paramMap.each { paramName, paramValue ->
                        currentSet[paramName] = paramValue
                        if (currentSet.size() == maxParallel) {
                            paramMapSets.add(currentSet)
                            currentSet = [:]
                        }
                    }

                    if (currentSet.size() > 0) {
                        paramMapSets.add(currentSet)
                    }

                    paramMapSets.each { paramMapSet ->
                        parallel paramMapSet.collect { paramName, paramValue ->
                            def currentColorCode = colorCodes[colorIndex % 2]
                            echo "\u001B[${currentColorCode}mRunning Playbook - ${paramName}: ${paramValue}\u001B[0m"

                            def playbookStep = "echo 'Running playbook for ${paramName}'" // Replace with your playbook command
                            if (playbookStep) {
                                parallel(
                                    "Execute Playbook - ${paramName}": {
                                        // Replace with your actual playbook execution logic
                                        sh playbookStep
                                    }
                                )
                            }

                            colorIndex++
                        }
                    }
                }
            }
        }
    }
}





// Define repositories and relevant files for Ansible repo
def codeRepo = "gitlab.com/codeRepo"
def ansibleRepo = "gitlab.com/ansibleRepo"
def relevantFilesForAnsible = ["build.groovy", "deploy.yml"]

// Function to process changesets
def processChangesets(repoUrl, relevantFiles = []) {
    def changeLogSets = currentBuild.changeSets
    def commList = ''

    for (entry in changeLogSets) {
        echo "${entry.id} by ${entry.author} on ${new Date(entry.timestamp)}: ${entry.msg}"

        // Check if the repository matches Ansible and if relevant files are specified
        if (repoUrl == ansibleRepo && !relevantFiles.isEmpty()) {
            // Check if the affected files include the relevant ones for Ansible repository
            def relevantChanges = entry.items.findAll { file ->
                relevantFiles.contains(file.path) || file.path.startsWith("Np-Deploy/")
            }

            if (!relevantChanges.isEmpty()) {
                commList = commList + "<li>${repoUrl}${entry.id}</li>"
                // Process affected files if needed
                for (file in relevantChanges) {
                    echo "  ${file.editType} ${file.path}"
                }
            }
        } else {
            // For other repositories or if no relevant files are specified, include all changes
            commList = commList + "<li>${repoUrl}${entry.id}</li>"
            // Process affected files if needed
            for (file in entry.items) {
                echo "  ${file.editType} ${file.path}"
            }
        }
    }
    return commList
}

// Process changesets for code repository
def codeCommList = processChangesets(codeRepo)

// Process changesets for Ansible repository with file filtering
def ansibleCommList = processChangesets(ansibleRepo, relevantFilesForAnsible)

// Now you have codeCommList for the code repository and ansibleCommList with only the relevant changesets for the specified files in the Ansible repository


sh """
COVERAGE_METRICS=$(curl -s -u SONAR_TOKEN: -X GET http://sonarhost/api/measures/component -d "component=<project_key>&metricKeys=coverage,uncovered_lines")
COVERAGE=$(echo $COVERAGE_METRICS | jq -r '.component.measures[] | select(.metric == "coverage") | .value')
UNCOVERED_LINES=$(echo $COVERAGE_METRICS | jq -r '.component.measures[] | select(.metric == "uncovered_lines") | .value')
"""



#!/bin/bash

# Directory paths
jar_dir="/apps/jars"
tar_dir="/apps/tars"
log_dir="/opt/Apache/sso/debug/logs"

# Function to keep 3 most recent files
keep_recent_3() {
    local dir=$1
    local file_ext=$2

    # Change to the directory
    cd "$dir" || return

    # List files by modification time, newest first
    files=(*$file_ext)
    IFS=$'\n' sorted_files=($(sort -r <<<"${files[*]}"))

    # Keep only the first 3, delete the rest
    for ((i=3; i<${#sorted_files[@]}; i++)); do
        echo "Deleting: ${sorted_files[$i]}"
        rm "${sorted_files[$i]}"
    done
}

# Call the function for each directory
keep_recent_3 "$jar_dir" "*.jar*"
keep_recent_3 "$tar_dir" "*.tar*"
keep_recent_3 "$log_dir" "*.log*"




import os
import gzip
import shutil
import boto3
import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configuration
EFS_PATH = "/mnt/efs"
S3_BUCKET = "your-s3-bucket-name"
LOG_FILE = "/mnt/efs/efs_cleanup.log"
EXCLUDE_FOLDERS = {"folder_to_exclude1", "folder_to_exclude2"}  # Folders to skip
MAX_DAYS = 7  # Keep last 7 days of data
MAX_WORKERS = 4  # Limit concurrent threads
AWS_REGION = "us-west-2"

# Setup logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

# Initialize S3 client
s3_client = boto3.client('s3', region_name=AWS_REGION)

def get_file_age_in_days(file_path):
    file_mtime = os.path.getmtime(file_path)
    file_age = (datetime.datetime.now() - datetime.datetime.fromtimestamp(file_mtime)).days
    return file_age

def compress_and_upload(file_path):
    try:
        compressed_path = file_path + ".gz"
        # Compress the file
        with open(file_path, 'rb') as f_in, gzip.open(compressed_path, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

        # Upload to S3
        s3_key = os.path.relpath(compressed_path, EFS_PATH)
        s3_client.upload_file(compressed_path, S3_BUCKET, s3_key)
        logging.info(f"Uploaded {compressed_path} to S3 bucket {S3_BUCKET} as {s3_key}")

        # Delete original file and compressed file locally
        os.remove(file_path)
        os.remove(compressed_path)
        logging.info(f"Deleted local files: {file_path} and {compressed_path}")
    except Exception as e:
        logging.error(f"Error compressing or uploading {file_path}: {e}")

def cleanup_folder(folder_path):
    try:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            for root, _, files in os.walk(folder_path):
                # Skip excluded folders
                if any(ex_folder in root for ex_folder in EXCLUDE_FOLDERS):
                    continue
                for file in files:
                    file_path = os.path.join(root, file)
                    if get_file_age_in_days(file_path) > MAX_DAYS:
                        futures.append(executor.submit(compress_and_upload, file_path))

            # Wait for all uploads to complete
            for future in as_completed(futures):
                future.result()
        logging.info(f"Cleanup completed for folder {folder_path}")
    except Exception as e:
        logging.error(f"Error cleaning up folder {folder_path}: {e}")

if __name__ == "__main__":
    logging.info("Starting EFS cleanup script")
    cleanup_folder(EFS_PATH)
    logging.info("EFS cleanup script completed")



#!/bin/bash

# Configuration
EFS_PATH="/mnt/efs"
S3_BUCKET="your-s3-bucket-name"
LOG_FILE="/mnt/efs/efs_cleanup.log"
EXCLUDE_FOLDERS=("folder_to_exclude1" "folder_to_exclude2")  # Specify folders to skip
MAX_DAYS=7  # Keep files from last 7 days
AWS_REGION="us-west-2"

# Start logging
echo "$(date): Starting EFS cleanup" | tee -a "$LOG_FILE"

# Loop through each folder in EFS_PATH to process files
find "$EFS_PATH" -type f ! -path "$EFS_PATH/${EXCLUDE_FOLDERS[0]}/*" ! -path "$EFS_PATH/${EXCLUDE_FOLDERS[1]}/*" -mtime +$MAX_DAYS | while read -r file; do

    # Compress the file
    gzip "$file"
    compressed_file="$file.gz"
    echo "$(date): Compressed $file to $compressed_file" | tee -a "$LOG_FILE"

    # Upload to S3
    s3_key="${compressed_file#$EFS_PATH/}"
    aws s3 cp "$compressed_file" "s3://$S3_BUCKET/$s3_key" --region "$AWS_REGION"
    if [[ $? -eq 0 ]]; then
        echo "$(date): Uploaded $compressed_file to s3://$S3_BUCKET/$s3_key" | tee -a "$LOG_FILE"
        # Delete the original file and compressed file from EFS after upload
        rm -f "$compressed_file"
        echo "$(date): Deleted local file $compressed_file" | tee -a "$LOG_FILE"
    else
        echo "$(date): Failed to upload $compressed_file to S3" | tee -a "$LOG_FILE"
    fi
done

echo "$(date): EFS cleanup completed" | tee -a "$LOG_FILE"



#!/bin/bash

# Limit CPU and I/O priority
# nice -n 10: Lower priority for CPU usage
# ionice -c2 -n7: Lower priority for disk I/O
exec nice -n 10 ionice -c2 -n7 bash "$0"

# Limit memory usage to 512MB
ulimit -v 524288

# Configuration
EFS_PATH="/mnt/efs"
S3_BUCKET="your-s3-bucket-name"
LOG_FILE="/mnt/efs/efs_cleanup.log"
EXCLUDE_FOLDERS=("folder_to_exclude1" "folder_to_exclude2")  # Specify folders to skip
MAX_DAYS=7  # Keep files from last 7 days
AWS_REGION="us-west-2"

# Start logging
echo "$(date): Starting EFS cleanup" | tee -a "$LOG_FILE"

# Loop through each folder in EFS_PATH to process files
find "$EFS_PATH" -type f ! -path "$EFS_PATH/${EXCLUDE_FOLDERS[0]}/*" ! -path "$EFS_PATH/${EXCLUDE_FOLDERS[1]}/*" -mtime +$MAX_DAYS | while read -r file; do

    # Compress the file
    gzip "$file"
    compressed_file="$file.gz"
    echo "$(date): Compressed $file to $compressed_file" | tee -a "$LOG_FILE"

    # Upload to S3
    s3_key="${compressed_file#$EFS_PATH/}"
    aws s3 cp "$compressed_file" "s3://$S3_BUCKET/$s3_key" --region "$AWS_REGION"
    if [[ $? -eq 0 ]]; then
        echo "$(date): Uploaded $compressed_file to s3://$S3_BUCKET/$s3_key" | tee -a "$LOG_FILE"
        # Delete the original file and compressed file from EFS after upload
        rm -f "$compressed_file"
        echo "$(date): Deleted local file $compressed_file" | tee -a "$LOG_FILE"
    else
        echo "$(date): Failed to upload $compressed_file to S3" | tee -a "$LOG_FILE"
    fi
done

echo "$(date): EFS cleanup completed" | tee -a "$LOG_FILE"


/mnt/efs/efs_cleanup.log {
    daily                     # Rotate the log file daily
    rotate 7                  # Keep only the last 7 rotations locally
    compress                  # Compress old logs to save space
    missingok                 # Ignore if the log file is missing
    notifempty                # Do not rotate empty log files
    delaycompress             # Delay compression until the next rotation
    copytruncate              # Truncate the log after rotation for continued logging

    # Post-rotation script to upload to S3 and delete local copy
    postrotate
        # Upload all compressed log files (except the latest 7 kept by `rotate`) to S3
        find /mnt/efs -name 'efs_cleanup.log.*.gz' -exec aws s3 cp {} s3://your-s3-bucket-name/logs/efs/ \; -exec rm {} \;
    endscript
}

ulimit -v 524288  # Limit virtual memory to 512MB
logrotate /etc/logrotate.conf


#!/bin/bash

# Base directory for test data
BASE_DIR="${HOME}/efs_test_data"
mkdir -p "$BASE_DIR"

# Variables for the test setup
NUM_DIRS=10       # Number of main directories to create
FILES_PER_DIR=20  # Number of files per directory
DEPTH=3           # Depth of nested directories
FILE_SIZE="10M"   # Size of each file (10MB for testing)

# Extensions for log files
EXTENSIONS=("log" "txt" "dat" "tmp")

echo "Creating test data in $BASE_DIR..."

# Function to create random data files
generate_files() {
    local target_dir=$1

    for ((i=1; i<=FILES_PER_DIR; i++)); do
        # Select random extension and filename
        extension=${EXTENSIONS[RANDOM % ${#EXTENSIONS[@]}]}
        filename="file_$i.$extension"
        
        # Generate file with random data
        dd if=/dev/urandom of="${target_dir}/${filename}" bs=1M count=$(echo $FILE_SIZE | tr -d 'M') status=none
        
        # Randomly assign old timestamps to simulate older files
        if (( RANDOM % 2 == 0 )); then
            touch -d "$((RANDOM % 30 + 30)) days ago" "${target_dir}/${filename}"
        fi
    done
}

# Recursive function to create nested directories and files
create_directories() {
    local current_depth=$1
    local current_dir=$2

    # Create files in the current directory
    generate_files "$current_dir"

    # If we haven't reached the max depth, create more directories
    if (( current_depth < DEPTH )); then
        for ((j=1; j<=NUM_DIRS; j++)); do
            new_dir="${current_dir}/subdir_$j"
            mkdir -p "$new_dir"
            create_directories $((current_depth + 1)) "$new_dir"
        done
    fi
}

# Start generating directories and files
create_directories 1 "$BASE_DIR"

echo "Test data creation complete in $BASE_DIR."